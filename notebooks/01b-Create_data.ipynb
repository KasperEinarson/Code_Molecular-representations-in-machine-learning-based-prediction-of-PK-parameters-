{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8128746-78c2-4a65-a425-8185ee1c4fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/python/conda/lib/python3.9/site-packages/Bio/SubsMat/__init__.py:126: BiopythonDeprecationWarning: Bio.SubsMat has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.substitution_matrices as a replacement, and contact the Biopython developers if you still need the Bio.SubsMat module.\n",
      "  warnings.warn(\n",
      "/home/kyei/.local/lib/python3.9/site-packages/gensim/matutils.py:24: DeprecationWarning: Please use `triu` from the `scipy.linalg` namespace, the `scipy.linalg.special_matrices` namespace is deprecated.\n",
      "  from scipy.linalg.special_matrices import triu\n",
      "/home/kyei/.local/lib/python3.9/site-packages/sklearn/utils/multiclass.py:14: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n",
      "/home/kyei/.local/lib/python3.9/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe2` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/home/kyei/.local/lib/python3.9/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe1` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "2022-11-21 10:24:36.311521: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-21 10:24:36.311578: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 01b-Create_data.ipynb to script\n",
      "[NbConvertApp] Writing 24665 bytes to 01b-Create_data.py\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Retrieve data from novo nordisk internal database both on descriptors and PK data.\n",
    "Select descriptors of interest and create embeddings from sequential data.\n",
    "Output: Exports final (full) dataset for further analysis.\n",
    "\n",
    "'''\n",
    "import novopy\n",
    "from novodataset.dataset import SAR4MLDataSet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from bio_embeddings.embed import ESM1bEmbedder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import sys\n",
    "import os\n",
    "from gensim.models import word2vec\n",
    "from rdkit import Chem\n",
    "from mol2vec.features import mol2alt_sentence, MolSentence, DfVec, sentences2vec\n",
    "from novodataset.dataset import SAR4MLDataSet\n",
    "w2v_model = word2vec.Word2Vec.load('../models/model_mol2vec.pkl')\n",
    "from ast import literal_eval\n",
    "\n",
    "# Set seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "!jupyter nbconvert --to script \"01b-Create_data.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed48a22d-4832-4c0f-a8b6-d1bd2e931f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functions needed for importing and selecting data appropriately for all descriptorsets + PK data.\n",
    "\n",
    "'''\n",
    "\n",
    "def select_descriptors_and_analogs(path = \"../data/processed/Descriptors_directly_from_nncd.csv\"):\n",
    "    '''\n",
    "    Function to load descriptors from csv output from \"01a-insilico_descriptors.py\" and return in format used for this analysis.\n",
    "    input: Path to descriptor csv file\n",
    "    output: Full_data - all descriptorsets for all insulin analogs used in this analysis.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Descriptors_directly_from_nncd.csv has been created by running python 01a-insilico_descriptors.py. Read SMILES as list instead of default strings:\n",
    "    Descriptors = pd.read_csv(path,converters={\"insilico2d:protractor:smiles\": literal_eval} )\n",
    "    Descriptors.set_index(\"nncno\",inplace=True)\n",
    "    Descriptors.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "    # DS1:\n",
    "    DS1_Descriptors = Descriptors.drop(Descriptors.columns[pd.Series(Descriptors.columns).str.startswith('insilico2d:protractor')],axis=1)\n",
    "    DS1_Descriptors.drop(\"seq\",axis=1,inplace=True)\n",
    "    DS1_Descriptors.columns = DS1_Descriptors.columns.str.replace('^insilico2d:', 'DS1_')\n",
    "    # DS2:\n",
    "    DS2_Descriptors = Descriptors[Descriptors.columns[pd.Series(Descriptors.columns).str.startswith('insilico2d:protractor')]]\n",
    "    DS2_Descriptors.drop(\"insilico2d:protractor:smiles\",axis=1,inplace=True)\n",
    "    DS2_Descriptors.columns = DS2_Descriptors.columns.str.replace('^insilico2d:protractor:', 'DS2_')\n",
    "    # DS3 (raw sequences):\n",
    "    DS3_raw = Descriptors[\"seq\"]\n",
    "    # DS4 (raw SMILES):\n",
    "    DS4_raw = Descriptors['insilico2d:protractor:smiles']\n",
    "    ## Small name adjustments:\n",
    "    DS1_Descriptors = DS1_Descriptors.rename({'ALOGP_PP': 'DS1_ALOGP_PP', 'PEP_BOND_COUNT': 'DS1_PEP_BOND_COUNT'}, axis=1) \n",
    "    \n",
    "    \n",
    "    \n",
    "    return DS1_Descriptors, DS2_Descriptors, DS3_raw, DS4_raw\n",
    "\n",
    "\n",
    "def load_PK_data(path):\n",
    "    '''\n",
    "    Load PK data, set index and remove invalid PK observations.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    PK = pd.read_excel(path)\n",
    "    PK.rename(columns={\"analouge\":\"nncno\"},inplace = True)\n",
    "    PK.set_index(\"nncno\",inplace=True)\n",
    "    PK = PK[PK['CL[ml/min/kg]'].notna()]\n",
    "    PK.drop(\"Vz[ml/kg]\",axis=1,inplace=True)\n",
    "    return PK\n",
    "\n",
    "\n",
    "def make_DS3_ESM_embedding(DS3_raw, save_path = \"../data/processed/pandas_ESM1b_Vivo.csv\"):\n",
    "    '''\n",
    "    Use ESM 1b embedding from bio embeddings module in python to create protein embedding\n",
    "    input: Raw amino acid sequence (un-aligned)\n",
    "    output: ESM1b embedding with 1280 dimensions.\n",
    "    '''\n",
    "    \n",
    "    ESM1b = ESM1bEmbedder()\n",
    "    ESM1b_embeddings = ESM1b.embed_many(DS3_raw)\n",
    "    ESM1b_trans = [ESM1bEmbedder.reduce_per_protein(e) for e in ESM1b_embeddings]\n",
    "    ESM1b_trans_pd = pd.DataFrame(ESM1b_trans)\n",
    "    ESM1b_trans_pd[\"nncno\"] = DS3_raw.index\n",
    "    ESM1b_trans_pd.to_csv(save_path)\n",
    "    return ESM1b_trans_pd\n",
    "\n",
    "\n",
    "def remove_None(SMILES_string):\n",
    "    '''\n",
    "    Some SMILES strings contains \"None\" which counts as an acylation (which is not correct..)\n",
    "    We therefore remove any None entries in the SMILES input\n",
    "    '''\n",
    "    DS4_raw_no_none = []\n",
    "    for i in range(DS4_raw.shape[0]):\n",
    "        DS4_raw_no_none.append([x for x in SMILES_string[i] if x is not None])\n",
    "    DS4_raw_no_none = pd.Series(DS4_raw_no_none,index = SMILES_string.index)\n",
    "    return DS4_raw_no_none\n",
    "    \n",
    "\n",
    "def make_DS4_embedding(SMILES_string, Concat = \"sum\"):\n",
    "    '''\n",
    "    Apologies for the hardcoding.\n",
    "    Function to calculate SMILES embedding of small molecule part. \n",
    "    In case of multiple protractors, the sum of the embeddings is calculated\n",
    "    Input: SMILES string of protractor\n",
    "    Input: Concat = How should multiple protractor embeddings be concatenated? options are \"mean\" or \"sum\".\n",
    "    output: DataFrame with embedding n x d, n = number of smiles string, d = embedding dimension. \n",
    "    \n",
    "    '''\n",
    "    SMILES_string_divided = SMILES_string.apply(lambda d: d if isinstance(d, list) else [])\n",
    "    # How many protracto?\n",
    "    lenght_prot = []\n",
    "    for i in range(len(SMILES_string_divided)):\n",
    "        lenght_prot.append(len(SMILES_string_divided[i]))\n",
    "\n",
    "    length_pd = pd.DataFrame(np.array(lenght_prot),columns = [\"Length\"],index = SMILES_string_divided.index)\n",
    "    #print(length_pd.value_counts())\n",
    "    # Handle 1 protractor \n",
    "    Pd_1_protractor = pd.DataFrame([SMILES_string_divided[SMILES_string_divided.index.isin(length_pd[length_pd.Length == 1].index)][i][0] for i in range(length_pd[length_pd.Length == 1].shape[0])],columns = [\"SMILES\"],index = length_pd[length_pd.Length == 1].index) \n",
    "    Pd_1_protractor[\"mol\"] = Pd_1_protractor[\"SMILES\"].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "    Pd_1_protractor['sentence'] = Pd_1_protractor.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], radius=1)), axis=1)\n",
    "    Pd_1_protractor['embedding'] = [DfVec(x) for x in sentences2vec(Pd_1_protractor['sentence'], w2v_model)]\n",
    "    Pd_1_protractor_embedding = np.array([x.vec for x in Pd_1_protractor['embedding']])\n",
    "    Pd_1_protractor_embedding = pd.DataFrame(Pd_1_protractor_embedding)\n",
    "    Pd_1_protractor_embedding.index = length_pd[length_pd.Length == 1].index\n",
    "    Pd_1_protractor_embedding = Pd_1_protractor_embedding.add_prefix('SMILES_')\n",
    "    # Handle zero protractors (easy..)\n",
    "    Pd_0_protractors = pd.DataFrame(np.zeros((length_pd[length_pd.Length == 0].shape[0], 100)),index = length_pd[length_pd.Length == 0].index,columns = Pd_1_protractor_embedding.columns)\n",
    "\n",
    "    # Handle 2 protractors\n",
    "    Pd_21_protractor = pd.DataFrame([SMILES_string_divided[SMILES_string_divided.index.isin(length_pd[length_pd.Length == 2].index)][i][0] for i in range(length_pd[length_pd.Length == 2].shape[0])],columns = [\"SMILES\"],index = length_pd[length_pd.Length == 2].index) \n",
    "    Pd_22_protractor = pd.DataFrame([SMILES_string_divided[SMILES_string_divided.index.isin(length_pd[length_pd.Length == 2].index)][i][1] for i in range(length_pd[length_pd.Length == 2].shape[0])],columns = [\"SMILES\"],index = length_pd[length_pd.Length == 2].index) \n",
    "\n",
    "    Pd_21_protractor[\"mol\"] = Pd_21_protractor[\"SMILES\"].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "    Pd_22_protractor[\"mol\"] = Pd_22_protractor[\"SMILES\"].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "\n",
    "    Pd_21_protractor['sentence'] = Pd_21_protractor.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], radius=1)), axis=1)\n",
    "    Pd_22_protractor['sentence'] = Pd_22_protractor.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], radius=1)), axis=1)\n",
    "\n",
    "    Pd_21_protractor['embedding'] = [DfVec(x) for x in sentences2vec(Pd_21_protractor['sentence'], w2v_model)]\n",
    "    Pd_22_protractor['embedding'] = [DfVec(x) for x in sentences2vec(Pd_22_protractor['sentence'], w2v_model)]\n",
    "\n",
    "    Pd_21_protractor_embedding = np.array([x.vec for x in Pd_21_protractor['embedding']])\n",
    "    Pd_22_protractor_embedding = np.array([x.vec for x in Pd_22_protractor['embedding']])\n",
    "\n",
    "    Pd_21_protractor_embedding = pd.DataFrame(Pd_21_protractor_embedding)\n",
    "    Pd_22_protractor_embedding = pd.DataFrame(Pd_22_protractor_embedding)\n",
    "\n",
    "    Pd_21_protractor_embedding.index = length_pd[length_pd.Length == 2].index\n",
    "    Pd_22_protractor_embedding.index = length_pd[length_pd.Length == 2].index\n",
    "\n",
    "    Pd_21_protractor_embedding = Pd_21_protractor_embedding.add_prefix('SMILES_').sort_index()\n",
    "    Pd_22_protractor_embedding = Pd_22_protractor_embedding.add_prefix('SMILES_').sort_index()\n",
    "\n",
    "\n",
    "    # Handle 3 protractors\n",
    "    Pd_31_protractor = pd.DataFrame([SMILES_string_divided[SMILES_string_divided.index.isin(length_pd[length_pd.Length == 3].index)][i][0] for i in range(length_pd[length_pd.Length == 3].shape[0])],columns = [\"SMILES\"],index = length_pd[length_pd.Length == 3].index) \n",
    "    Pd_32_protractor = pd.DataFrame([SMILES_string_divided[SMILES_string_divided.index.isin(length_pd[length_pd.Length == 3].index)][i][1] for i in range(length_pd[length_pd.Length == 3].shape[0])],columns = [\"SMILES\"],index = length_pd[length_pd.Length == 3].index) \n",
    "    Pd_33_protractor = pd.DataFrame([SMILES_string_divided[SMILES_string_divided.index.isin(length_pd[length_pd.Length == 3].index)][i][2] for i in range(length_pd[length_pd.Length == 3].shape[0])],columns = [\"SMILES\"],index = length_pd[length_pd.Length == 3].index) \n",
    " \n",
    "    Pd_31_protractor[\"mol\"] = Pd_31_protractor[\"SMILES\"].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "    Pd_32_protractor[\"mol\"] = Pd_32_protractor[\"SMILES\"].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "    Pd_33_protractor[\"mol\"] = Pd_33_protractor[\"SMILES\"].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "\n",
    "    Pd_31_protractor['sentence'] = Pd_31_protractor.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], radius=1)), axis=1)\n",
    "    Pd_32_protractor['sentence'] = Pd_32_protractor.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], radius=1)), axis=1)\n",
    "    Pd_33_protractor['sentence'] = Pd_33_protractor.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], radius=1)), axis=1)\n",
    "\n",
    "    Pd_31_protractor['embedding'] = [DfVec(x) for x in sentences2vec(Pd_31_protractor['sentence'], w2v_model)]\n",
    "    Pd_32_protractor['embedding'] = [DfVec(x) for x in sentences2vec(Pd_32_protractor['sentence'], w2v_model)]\n",
    "    Pd_33_protractor['embedding'] = [DfVec(x) for x in sentences2vec(Pd_33_protractor['sentence'], w2v_model)]\n",
    "\n",
    "    Pd_31_protractor_embedding = np.array([x.vec for x in Pd_31_protractor['embedding']])\n",
    "    Pd_32_protractor_embedding = np.array([x.vec for x in Pd_32_protractor['embedding']])\n",
    "    Pd_33_protractor_embedding = np.array([x.vec for x in Pd_33_protractor['embedding']])\n",
    "\n",
    "    Pd_31_protractor_embedding = pd.DataFrame(Pd_31_protractor_embedding)\n",
    "    Pd_32_protractor_embedding = pd.DataFrame(Pd_32_protractor_embedding)\n",
    "    Pd_33_protractor_embedding = pd.DataFrame(Pd_33_protractor_embedding)\n",
    "\n",
    "    Pd_31_protractor_embedding.index = length_pd[length_pd.Length == 3].index\n",
    "    Pd_32_protractor_embedding.index = length_pd[length_pd.Length == 3].index\n",
    "    Pd_33_protractor_embedding.index = length_pd[length_pd.Length == 3].index\n",
    "\n",
    "    Pd_31_protractor_embedding = Pd_31_protractor_embedding.add_prefix('SMILES_').sort_index()\n",
    "    Pd_32_protractor_embedding = Pd_32_protractor_embedding.add_prefix('SMILES_').sort_index()\n",
    "    Pd_33_protractor_embedding = Pd_33_protractor_embedding.add_prefix('SMILES_').sort_index()\n",
    "\n",
    "    \n",
    "    # Handle 4 protractors:\n",
    "    \n",
    "    Pd_41_protractor = pd.DataFrame([SMILES_string_divided[SMILES_string_divided.index.isin(length_pd[length_pd.Length == 4].index)][i][0] for i in range(length_pd[length_pd.Length == 4].shape[0])],columns = [\"SMILES\"],index = length_pd[length_pd.Length == 4].index) \n",
    "    Pd_42_protractor = pd.DataFrame([SMILES_string_divided[SMILES_string_divided.index.isin(length_pd[length_pd.Length == 4].index)][i][1] for i in range(length_pd[length_pd.Length == 4].shape[0])],columns = [\"SMILES\"],index = length_pd[length_pd.Length == 4].index) \n",
    "    Pd_43_protractor = pd.DataFrame([SMILES_string_divided[SMILES_string_divided.index.isin(length_pd[length_pd.Length == 4].index)][i][2] for i in range(length_pd[length_pd.Length == 4].shape[0])],columns = [\"SMILES\"],index = length_pd[length_pd.Length == 4].index) \n",
    "    Pd_44_protractor = pd.DataFrame([SMILES_string_divided[SMILES_string_divided.index.isin(length_pd[length_pd.Length == 4].index)][i][3] for i in range(length_pd[length_pd.Length == 4].shape[0])],columns = [\"SMILES\"],index = length_pd[length_pd.Length == 4].index) \n",
    " \n",
    "    Pd_41_protractor[\"mol\"] = Pd_41_protractor[\"SMILES\"].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "    Pd_42_protractor[\"mol\"] = Pd_42_protractor[\"SMILES\"].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "    Pd_43_protractor[\"mol\"] = Pd_43_protractor[\"SMILES\"].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "    Pd_44_protractor[\"mol\"] = Pd_44_protractor[\"SMILES\"].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "\n",
    "    Pd_41_protractor['sentence'] = Pd_41_protractor.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], radius=1)), axis=1)\n",
    "    Pd_42_protractor['sentence'] = Pd_42_protractor.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], radius=1)), axis=1)\n",
    "    Pd_43_protractor['sentence'] = Pd_43_protractor.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], radius=1)), axis=1)\n",
    "    Pd_44_protractor['sentence'] = Pd_44_protractor.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], radius=1)), axis=1)\n",
    "\n",
    "    Pd_41_protractor['embedding'] = [DfVec(x) for x in sentences2vec(Pd_41_protractor['sentence'], w2v_model)]\n",
    "    Pd_42_protractor['embedding'] = [DfVec(x) for x in sentences2vec(Pd_42_protractor['sentence'], w2v_model)]\n",
    "    Pd_43_protractor['embedding'] = [DfVec(x) for x in sentences2vec(Pd_43_protractor['sentence'], w2v_model)]\n",
    "    Pd_44_protractor['embedding'] = [DfVec(x) for x in sentences2vec(Pd_44_protractor['sentence'], w2v_model)]\n",
    "\n",
    "    \n",
    "    Pd_41_protractor_embedding = np.array([x.vec for x in Pd_41_protractor['embedding']])\n",
    "    Pd_42_protractor_embedding = np.array([x.vec for x in Pd_42_protractor['embedding']])\n",
    "    Pd_43_protractor_embedding = np.array([x.vec for x in Pd_43_protractor['embedding']])\n",
    "    Pd_44_protractor_embedding = np.array([x.vec for x in Pd_44_protractor['embedding']])\n",
    "\n",
    "    Pd_41_protractor_embedding = pd.DataFrame(Pd_41_protractor_embedding)\n",
    "    Pd_42_protractor_embedding = pd.DataFrame(Pd_42_protractor_embedding)\n",
    "    Pd_43_protractor_embedding = pd.DataFrame(Pd_43_protractor_embedding)\n",
    "    Pd_44_protractor_embedding = pd.DataFrame(Pd_44_protractor_embedding)\n",
    "\n",
    "    Pd_41_protractor_embedding.index = length_pd[length_pd.Length == 4].index\n",
    "    Pd_42_protractor_embedding.index = length_pd[length_pd.Length == 4].index\n",
    "    Pd_43_protractor_embedding.index = length_pd[length_pd.Length == 4].index\n",
    "    Pd_44_protractor_embedding.index = length_pd[length_pd.Length == 4].index\n",
    "\n",
    "    Pd_41_protractor_embedding = Pd_41_protractor_embedding.add_prefix('SMILES_').sort_index()\n",
    "    Pd_42_protractor_embedding = Pd_42_protractor_embedding.add_prefix('SMILES_').sort_index()\n",
    "    Pd_43_protractor_embedding = Pd_43_protractor_embedding.add_prefix('SMILES_').sort_index()\n",
    "    Pd_44_protractor_embedding = Pd_44_protractor_embedding.add_prefix('SMILES_').sort_index()\n",
    "\n",
    "    \n",
    "    \n",
    "     # Handle 7 protractors:\n",
    "    \n",
    "    Pd_71_protractor = pd.DataFrame([SMILES_string_divided[SMILES_string_divided.index.isin(length_pd[length_pd.Length == 7].index)][i][0] for i in range(length_pd[length_pd.Length == 7].shape[0])],columns = [\"SMILES\"],index = length_pd[length_pd.Length == 7].index) \n",
    "    Pd_72_protractor = pd.DataFrame([SMILES_string_divided[SMILES_string_divided.index.isin(length_pd[length_pd.Length == 7].index)][i][1] for i in range(length_pd[length_pd.Length == 7].shape[0])],columns = [\"SMILES\"],index = length_pd[length_pd.Length == 7].index) \n",
    "    Pd_73_protractor = pd.DataFrame([SMILES_string_divided[SMILES_string_divided.index.isin(length_pd[length_pd.Length == 7].index)][i][2] for i in range(length_pd[length_pd.Length == 7].shape[0])],columns = [\"SMILES\"],index = length_pd[length_pd.Length == 7].index) \n",
    "    Pd_74_protractor = pd.DataFrame([SMILES_string_divided[SMILES_string_divided.index.isin(length_pd[length_pd.Length == 7].index)][i][3] for i in range(length_pd[length_pd.Length == 7].shape[0])],columns = [\"SMILES\"],index = length_pd[length_pd.Length == 7].index) \n",
    "    Pd_75_protractor = pd.DataFrame([SMILES_string_divided[SMILES_string_divided.index.isin(length_pd[length_pd.Length == 7].index)][i][4] for i in range(length_pd[length_pd.Length == 7].shape[0])],columns = [\"SMILES\"],index = length_pd[length_pd.Length == 7].index) \n",
    "    Pd_76_protractor = pd.DataFrame([SMILES_string_divided[SMILES_string_divided.index.isin(length_pd[length_pd.Length == 7].index)][i][5] for i in range(length_pd[length_pd.Length == 7].shape[0])],columns = [\"SMILES\"],index = length_pd[length_pd.Length == 7].index) \n",
    "    Pd_77_protractor = pd.DataFrame([SMILES_string_divided[SMILES_string_divided.index.isin(length_pd[length_pd.Length == 7].index)][i][6] for i in range(length_pd[length_pd.Length == 7].shape[0])],columns = [\"SMILES\"],index = length_pd[length_pd.Length == 7].index) \n",
    " \n",
    "    Pd_71_protractor[\"mol\"] = Pd_71_protractor[\"SMILES\"].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "    Pd_72_protractor[\"mol\"] = Pd_72_protractor[\"SMILES\"].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "    Pd_73_protractor[\"mol\"] = Pd_73_protractor[\"SMILES\"].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "    Pd_74_protractor[\"mol\"] = Pd_74_protractor[\"SMILES\"].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "    Pd_75_protractor[\"mol\"] = Pd_75_protractor[\"SMILES\"].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "    Pd_76_protractor[\"mol\"] = Pd_76_protractor[\"SMILES\"].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "    Pd_77_protractor[\"mol\"] = Pd_77_protractor[\"SMILES\"].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "\n",
    "    Pd_71_protractor['sentence'] = Pd_71_protractor.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], radius=1)), axis=1)\n",
    "    Pd_72_protractor['sentence'] = Pd_72_protractor.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], radius=1)), axis=1)\n",
    "    Pd_73_protractor['sentence'] = Pd_73_protractor.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], radius=1)), axis=1)\n",
    "    Pd_74_protractor['sentence'] = Pd_74_protractor.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], radius=1)), axis=1)\n",
    "    Pd_75_protractor['sentence'] = Pd_75_protractor.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], radius=1)), axis=1)\n",
    "    Pd_76_protractor['sentence'] = Pd_76_protractor.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], radius=1)), axis=1)\n",
    "    Pd_77_protractor['sentence'] = Pd_77_protractor.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], radius=1)), axis=1)\n",
    "\n",
    "    Pd_71_protractor['embedding'] = [DfVec(x) for x in sentences2vec(Pd_71_protractor['sentence'], w2v_model)]\n",
    "    Pd_72_protractor['embedding'] = [DfVec(x) for x in sentences2vec(Pd_72_protractor['sentence'], w2v_model)]\n",
    "    Pd_73_protractor['embedding'] = [DfVec(x) for x in sentences2vec(Pd_73_protractor['sentence'], w2v_model)]\n",
    "    Pd_74_protractor['embedding'] = [DfVec(x) for x in sentences2vec(Pd_74_protractor['sentence'], w2v_model)]\n",
    "    Pd_75_protractor['embedding'] = [DfVec(x) for x in sentences2vec(Pd_75_protractor['sentence'], w2v_model)]\n",
    "    Pd_76_protractor['embedding'] = [DfVec(x) for x in sentences2vec(Pd_76_protractor['sentence'], w2v_model)]\n",
    "    Pd_77_protractor['embedding'] = [DfVec(x) for x in sentences2vec(Pd_77_protractor['sentence'], w2v_model)]\n",
    "\n",
    "    \n",
    "    Pd_71_protractor_embedding = np.array([x.vec for x in Pd_71_protractor['embedding']])\n",
    "    Pd_72_protractor_embedding = np.array([x.vec for x in Pd_72_protractor['embedding']])\n",
    "    Pd_73_protractor_embedding = np.array([x.vec for x in Pd_73_protractor['embedding']])\n",
    "    Pd_74_protractor_embedding = np.array([x.vec for x in Pd_74_protractor['embedding']])\n",
    "    Pd_75_protractor_embedding = np.array([x.vec for x in Pd_75_protractor['embedding']])\n",
    "    Pd_76_protractor_embedding = np.array([x.vec for x in Pd_76_protractor['embedding']])\n",
    "    Pd_77_protractor_embedding = np.array([x.vec for x in Pd_77_protractor['embedding']])\n",
    "\n",
    "    Pd_71_protractor_embedding = pd.DataFrame(Pd_71_protractor_embedding)\n",
    "    Pd_72_protractor_embedding = pd.DataFrame(Pd_72_protractor_embedding)\n",
    "    Pd_73_protractor_embedding = pd.DataFrame(Pd_73_protractor_embedding)\n",
    "    Pd_74_protractor_embedding = pd.DataFrame(Pd_74_protractor_embedding)\n",
    "    Pd_75_protractor_embedding = pd.DataFrame(Pd_75_protractor_embedding)\n",
    "    Pd_76_protractor_embedding = pd.DataFrame(Pd_76_protractor_embedding)\n",
    "    Pd_77_protractor_embedding = pd.DataFrame(Pd_77_protractor_embedding)\n",
    "\n",
    "    Pd_71_protractor_embedding.index = length_pd[length_pd.Length == 7].index\n",
    "    Pd_72_protractor_embedding.index = length_pd[length_pd.Length == 7].index\n",
    "    Pd_73_protractor_embedding.index = length_pd[length_pd.Length == 7].index\n",
    "    Pd_74_protractor_embedding.index = length_pd[length_pd.Length == 7].index\n",
    "    Pd_75_protractor_embedding.index = length_pd[length_pd.Length == 7].index\n",
    "    Pd_76_protractor_embedding.index = length_pd[length_pd.Length == 7].index\n",
    "    Pd_77_protractor_embedding.index = length_pd[length_pd.Length == 7].index\n",
    "\n",
    "    Pd_71_protractor_embedding = Pd_71_protractor_embedding.add_prefix('SMILES_').sort_index()\n",
    "    Pd_72_protractor_embedding = Pd_72_protractor_embedding.add_prefix('SMILES_').sort_index()\n",
    "    Pd_73_protractor_embedding = Pd_73_protractor_embedding.add_prefix('SMILES_').sort_index()\n",
    "    Pd_74_protractor_embedding = Pd_74_protractor_embedding.add_prefix('SMILES_').sort_index()\n",
    "    Pd_75_protractor_embedding = Pd_75_protractor_embedding.add_prefix('SMILES_').sort_index()\n",
    "    Pd_76_protractor_embedding = Pd_76_protractor_embedding.add_prefix('SMILES_').sort_index()\n",
    "    Pd_77_protractor_embedding = Pd_77_protractor_embedding.add_prefix('SMILES_').sort_index()\n",
    "\n",
    "    \n",
    "    \n",
    "    # Concatenation of multiple protractors (mean or sum?)\n",
    "    if Concat == \"sum\":\n",
    "        Pd_2_protractor_embeddingp_mean = pd.concat([Pd_21_protractor_embedding, Pd_22_protractor_embedding]).groupby(level=0).sum()\n",
    "        Pd_3_protractor_embeddingp_mean = pd.concat([Pd_31_protractor_embedding, Pd_32_protractor_embedding, Pd_33_protractor_embedding]).groupby(level=0).sum()\n",
    "        Pd_4_protractor_embeddingp_mean = pd.concat([Pd_41_protractor_embedding, Pd_42_protractor_embedding, Pd_43_protractor_embedding,Pd_44_protractor_embedding]).groupby(level=0).sum()\n",
    "        Pd_7_protractor_embeddingp_mean = pd.concat([Pd_71_protractor_embedding, Pd_72_protractor_embedding, Pd_73_protractor_embedding,Pd_74_protractor_embedding,Pd_75_protractor_embedding,Pd_76_protractor_embedding,Pd_77_protractor_embedding ]).groupby(level=0).sum()\n",
    "    elif Concat == \"mean\":\n",
    "        Pd_2_protractor_embeddingp_mean = pd.concat([Pd_21_protractor_embedding, Pd_22_protractor_embedding]).groupby(level=0).mean()\n",
    "        Pd_3_protractor_embeddingp_mean = pd.concat([Pd_31_protractor_embedding, Pd_32_protractor_embedding, Pd_33_protractor_embedding]).groupby(level=0).mean()\n",
    "        Pd_4_protractor_embeddingp_mean = pd.concat([Pd_41_protractor_embedding, Pd_42_protractor_embedding, Pd_43_protractor_embedding,Pd_44_protractor_embedding]).groupby(level=0).mean()\n",
    "        Pd_7_protractor_embeddingp_mean = pd.concat([Pd_71_protractor_embedding, Pd_72_protractor_embedding, Pd_73_protractor_embedding,Pd_74_protractor_embedding,Pd_75_protractor_embedding,Pd_76_protractor_embedding,Pd_77_protractor_embedding ]).groupby(level=0).mean()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Concatenate all data together:\n",
    "    SMILES_word2vec_embedding = pd.concat([Pd_0_protractors,Pd_1_protractor_embedding, Pd_2_protractor_embeddingp_mean,Pd_3_protractor_embeddingp_mean,Pd_4_protractor_embeddingp_mean,Pd_7_protractor_embeddingp_mean])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return SMILES_word2vec_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83eaa0d-5ecd-4a97-87d6-b08a712b1ae6",
   "metadata": {},
   "source": [
    "# Load PK data and descriptors for all analogs using above functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecfc88f5-ab27-44c7-bdde-f5c4b68a03be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88408/1285817092.py:21: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  DS1_Descriptors.columns = DS1_Descriptors.columns.str.replace('^insilico2d:', 'DS1_')\n",
      "/tmp/ipykernel_88408/1285817092.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  DS2_Descriptors.drop(\"insilico2d:protractor:smiles\",axis=1,inplace=True)\n",
      "/tmp/ipykernel_88408/1285817092.py:25: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  DS2_Descriptors.columns = DS2_Descriptors.columns.str.replace('^insilico2d:protractor:', 'DS2_')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DS1, DS2, DS3_raw, DS4_raw = select_descriptors_and_analogs(path = \"../data/processed/Descriptors_directly_from_nncd.csv\")\n",
    "PK_data = load_PK_data(\"PK_data_properitary.csv\")\n",
    "PK_data = PK_data[PK_data.index.isin(DS1.index)]\n",
    "DS4_raw_no_none = remove_None(DS4_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70840e86-d565-4f35-be8d-21f5e0b16aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88408/3157316522.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  DS4.columns = DS4.columns.str.replace('^SMILES', 'DS4')\n"
     ]
    }
   ],
   "source": [
    "# Make DS3 embedding:\n",
    "# Takes DS3_raw and produces the embedding. Following line of code takes ~30 minutes to run on standard laptop and is therefore saved as a seperate data-file once processed.\n",
    "#DS3 = make_DS3_ESM_embedding(DS3_raw,save_path = \"../data/processed/pandas_ESM1b_Vivo.csv\")\n",
    "DS3 = pd.read_csv(\"../data/processed/pandas_ESM1b_Vivo.csv\")\n",
    "DS3.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "DS3.set_index(\"nncno\",inplace=True)                  \n",
    "DS3 = DS3.add_prefix('DS3_')\n",
    "\n",
    "# Make DS4 embedding:\n",
    "DS4 = make_DS4_embedding(DS4_raw_no_none,Concat = \"sum\")\n",
    "DS4.columns = DS4.columns.str.replace('^SMILES', 'DS4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a4e3dd4-a9a7-4827-b9c7-1511919efbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export full dataset (all descriptors + PK data)\n",
    "Full_data = pd.concat([DS1,DS2,DS3,DS4,PK_data],axis=1)\n",
    "# Last row is all NAs:\n",
    "Full_data = Full_data[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98150a6a-b25c-4ea1-b0c5-6813a85aa730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the full data set to csv:\n",
    "Full_data.to_csv(\"../data/processed/full_data_set.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
