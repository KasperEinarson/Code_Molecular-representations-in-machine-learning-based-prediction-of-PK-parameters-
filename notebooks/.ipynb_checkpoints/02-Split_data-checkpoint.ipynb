{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45a8516a-6099-4eee-a6ec-481762475daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 02-Split_data.ipynb to script\n",
      "[NbConvertApp] Writing 7746 bytes to 02-Split_data.py\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Notebook to split data into a user defined amount of folds for training, validation and test. This is done for the full descriptorset (DS1234) such that subsets of descriptors can be extracted in the model notebooks (RF, ANN).\n",
    "Splitting the data here is only used for ANN model later on. The exact same splitting is done within \"Random_Forest.ipynb\" for RF model seperately.\n",
    "Output: List of dataframes exported to folder \"Processed data\" which contains all relevant data splitted and scaled for each fold and ready as model (ANN) input.\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupKFold,GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../src/insulin_pk/utils/') \n",
    "from utils import *\n",
    "!jupyter nbconvert --to script \"02-Split_data.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3a3550b-8859-4423-a953-3d7a77368486",
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_data = pd.read_csv(\"../data/processed/full_data_set.csv\")\n",
    "Full_data.set_index(\"nncno\",inplace=True)\n",
    "PK_names = [\"CL[ml/min/kg]\",\"T1/2[h]\",\"MRT[h]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8754300c-bf23-4a5d-99d1-847890284e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Full_data.drop(PK_names,axis=1)\n",
    "Y = Full_data[PK_names]\n",
    "Y = np.log(Y)\n",
    "\n",
    "\n",
    "# Initialize empty list to fill up with train,val and test data for each fold for the full descriptorset\n",
    "data_folds = []\n",
    "# Split data into user defined cross validation folds:\n",
    "# Note that this splitting is identical to the splitting in \"Random Forest.ipynb\" in order to be able to compare results between RF and ANN using paired t-test later on.\n",
    "idx_outer = list(GroupKFold(n_splits = 5).split(Full_data,groups = Full_data.index.get_level_values(\"nncno\")))\n",
    "for i in range(len(idx_outer)): \n",
    "                X_train_outer, X_test, Y_train_outer, Y_test = X.iloc[idx_outer[i][0]], X.iloc[idx_outer[i][1]], Y.iloc[idx_outer[i][0]], Y.iloc[idx_outer[i][1]]\n",
    "                # Start Inner split\n",
    "                idx_inner = list(GroupShuffleSplit(n_splits = 1, test_size = 0.15, random_state = 42).split(X_train_outer,groups = X_train_outer.index.get_level_values(\"nncno\")))[0]\n",
    "                X_train, X_val, Y_train, Y_val = X_train_outer.iloc[idx_inner[0]], X_train_outer.iloc[idx_inner[1]], Y_train_outer.iloc[idx_inner[0]], Y_train_outer.iloc[idx_inner[1]]\n",
    "                \n",
    "                ## Scale data appropriately\n",
    "                scaler_X = StandardScaler().fit(X_train)\n",
    "                scaler_Y = StandardScaler().fit(Y_train)\n",
    "                \n",
    "                ### ------ scale X\n",
    "                X_train_scaled = pd.DataFrame(scaler_X.transform(X_train),columns = X_train.columns)\n",
    "                X_train_scaled = X_train_scaled.set_index(X_train.index)\n",
    "                \n",
    "                X_val_scaled = pd.DataFrame(scaler_X.transform(X_val),columns = X_val.columns)\n",
    "                X_val_scaled = X_val_scaled.set_index(X_val.index)\n",
    "    \n",
    "                X_test_scaled = pd.DataFrame(scaler_X.transform(X_test),columns = X_test.columns)\n",
    "                X_test_scaled = X_test_scaled.set_index(X_test.index)\n",
    "                ### ------ scale Y\n",
    "                \n",
    "                Y_train_scaled = pd.DataFrame(scaler_Y.transform(Y_train),index = Y_train.index,columns = PK_names)\n",
    "                Y_val_scaled = pd.DataFrame(scaler_Y.transform(Y_val),index = Y_val.index,columns = PK_names)\n",
    "                Y_test_scaled = pd.DataFrame(scaler_Y.transform(Y_test),index = Y_test.index,columns = PK_names)\n",
    "                ### Append to list for export:\n",
    "                data_folds.append([X_train_scaled,X_val_scaled,X_test_scaled,Y_train_scaled,Y_val_scaled,Y_test_scaled])\n",
    "                ### Export Y scaler for later reconstruction of original PK values:\n",
    "                with open('../data/processed/Scaler_Y_' + \"{0}.pkl\".format(i),'wb') as f:pickle.dump(scaler_Y,f )  \n",
    "\n",
    "with open('../data/processed/Data_folds.pkl','wb') as f:pickle.dump(data_folds,f )             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c5b968-42d6-4f3c-8dea-1e81cf609e8c",
   "metadata": {},
   "source": [
    "# Grouped data for ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e876df3-b7c0-4f86-a655-2929a32d9f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create data with groups\n",
    "Data_with_groups = pd.read_excel(\"../data/raw/Data_with_groups.xlsx\")\n",
    "Data_with_groups.rename(columns={\"NNCNo\": \"nncno\"})\n",
    "Data_with_groups.set_index(\"NNCNo\",inplace=True)\n",
    "Data_with_groups = Data_with_groups[~Data_with_groups.index.isin([\"0148-0000-1247\"])]\n",
    "Full_data = pd.read_csv(\"../data/processed/full_data_set.csv\")\n",
    "Full_data.set_index(\"nncno\",inplace=True)\n",
    "PK_names = [\"CL[ml/min/kg]\",\"T1/2[h]\",\"MRT[h]\"]\n",
    "data_wg = pd.merge(Full_data, Data_with_groups[\"Groups\"], left_index=True, right_index=True)\n",
    "data_wg.index.name = \"nncno\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1997d8ab-09dc-455a-a8bc-75583a9e8958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_585/2294794642.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Full_data.drop(\"Groups\",axis=1,inplace=True)\n",
      "/tmp/ipykernel_585/2294794642.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Full_data.drop(\"Groups\",axis=1,inplace=True)\n",
      "/tmp/ipykernel_585/2294794642.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Full_data.drop(\"Groups\",axis=1,inplace=True)\n",
      "/tmp/ipykernel_585/2294794642.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Full_data.drop(\"Groups\",axis=1,inplace=True)\n",
      "/tmp/ipykernel_585/2294794642.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Full_data.drop(\"Groups\",axis=1,inplace=True)\n",
      "/tmp/ipykernel_585/2294794642.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Full_data.drop(\"Groups\",axis=1,inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "groups = [\"Other\", 'Acylation', 'No attachments', 'Concatenated proteins',\n",
    "       'Antibody attachment (Fc)', 'AA extensions']     \n",
    "\n",
    "for kk in groups:\n",
    "    Full_data = data_wg[data_wg[\"Groups\"] == kk]\n",
    "    Full_data.drop(\"Groups\",axis=1,inplace=True)\n",
    "    X = Full_data.drop(PK_names,axis=1)\n",
    "    Y = Full_data[PK_names]\n",
    "    Y = np.log(Y)\n",
    "\n",
    "    # Initialize empty list to fill up with train,val and test data for each fold for the full descriptorset\n",
    "    data_folds = []\n",
    "    # Split data into user defined cross validation folds:\n",
    "    # Note that this splitting is identical to the splitting in \"Random Forest.ipynb\" in order to be able to compare results between RF and ANN using paired t-test later on.\n",
    "    idx_outer = list(GroupKFold(n_splits = 5).split(Full_data,groups = Full_data.index.get_level_values(\"nncno\")))\n",
    "    for i in range(len(idx_outer)): \n",
    "                    X_train_outer, X_test, Y_train_outer, Y_test = X.iloc[idx_outer[i][0]], X.iloc[idx_outer[i][1]], Y.iloc[idx_outer[i][0]], Y.iloc[idx_outer[i][1]]\n",
    "                    # Start Inner split\n",
    "                    idx_inner = list(GroupShuffleSplit(n_splits = 1, test_size = 0.15, random_state = 42).split(X_train_outer,groups = X_train_outer.index.get_level_values(\"nncno\")))[0]\n",
    "                    X_train, X_val, Y_train, Y_val = X_train_outer.iloc[idx_inner[0]], X_train_outer.iloc[idx_inner[1]], Y_train_outer.iloc[idx_inner[0]], Y_train_outer.iloc[idx_inner[1]]\n",
    "\n",
    "                    ## Scale data appropriately\n",
    "                    scaler_X = StandardScaler().fit(X_train)\n",
    "                    scaler_Y = StandardScaler().fit(Y_train)\n",
    "\n",
    "                    ### ------ scale X\n",
    "                    X_train_scaled = pd.DataFrame(scaler_X.transform(X_train),columns = X_train.columns)\n",
    "                    X_train_scaled = X_train_scaled.set_index(X_train.index)\n",
    "\n",
    "                    X_val_scaled = pd.DataFrame(scaler_X.transform(X_val),columns = X_val.columns)\n",
    "                    X_val_scaled = X_val_scaled.set_index(X_val.index)\n",
    "\n",
    "                    X_test_scaled = pd.DataFrame(scaler_X.transform(X_test),columns = X_test.columns)\n",
    "                    X_test_scaled = X_test_scaled.set_index(X_test.index)\n",
    "                    ### ------ scale Y\n",
    "\n",
    "                    Y_train_scaled = pd.DataFrame(scaler_Y.transform(Y_train),index = Y_train.index,columns = PK_names)\n",
    "                    Y_val_scaled = pd.DataFrame(scaler_Y.transform(Y_val),index = Y_val.index,columns = PK_names)\n",
    "                    Y_test_scaled = pd.DataFrame(scaler_Y.transform(Y_test),index = Y_test.index,columns = PK_names)\n",
    "                    ### Append to list for export:\n",
    "                    data_folds.append([X_train_scaled,X_val_scaled,X_test_scaled,Y_train_scaled,Y_val_scaled,Y_test_scaled])\n",
    "                    ### Export Y scaler for later reconstruction of original PK values:\n",
    "                    with open('../data/processed/Scaler_Y_{}_{}.pkl'.format(kk,i),'wb') as f:pickle.dump(scaler_Y,f )  \n",
    "\n",
    "    with open('../data/processed/Data_folds_{}.pkl'.format(kk),'wb') as f:pickle.dump(data_folds,f )             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cef6968f-ecae-4e04-9fda-eacd72fbe181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other\n",
      "44\n",
      "Acylation\n",
      "338\n",
      "No attachments\n",
      "9\n",
      "Concatenated proteins\n",
      "154\n",
      "Antibody attachment (Fc)\n",
      "60\n",
      "AA extensions\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "for kk in groups:\n",
    "    dat_tmp = pickle.load(open('../data/processed/Data_folds_{}.pkl'.format(kk),'rb'))\n",
    "    print(kk)\n",
    "    print(dat_tmp[0][0].shape[0] + dat_tmp[0][1].shape[0] + dat_tmp[0][2].shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
